name: Daily GPT-5 Stock Alert to Discord

on:
  schedule:
    - cron: '0 4 * * *' # Runs every day at 04:00 UTC.
  workflow_dispatch: # Allows manual runs from the Actions tab.

permissions:
  contents: write # Lets the workflow commit the logs file back to the repo.

jobs:
  send-alert:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          persist-credentials: true # Reuses the token for push operations.
          fetch-depth: 0 # Fetches full history so commits work cleanly.

      - name: Restore cached data (aliases, universe, etc.)
        uses: actions/cache@v4
        with:
          path: data # Caches the data folder to save API calls and time.
          key: universe-cache-${{ github.run_id }} # Uses a unique cache key per run.
          restore-keys: |
            universe-cache-           # Falls back to the most recent cache if exact key missing.
      - name: Purge valuation caches
        run: |
          rm -f data/pe_cache.json data/valuations_cache.json

      # Create a minimal requirements.txt so setup-python's pip cache has a key
      - name: Ensure dependency manifest
        run: |
          if [ ! -f requirements.txt ]; then
            printf "numpy\npandas\nyfinance\nrequests\npytz\n" > requirements.txt
          fi
          echo "Using requirements.txt:"
          cat requirements.txt

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11' # Chooses Python 3.11 for faster runtime and newer libs.
          cache-dependency-path: requirements.txt # Enables pip caching keyed by this file.

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install --upgrade --upgrade-strategy eager numpy pandas yfinance requests pytz

      - name: Verify environment (debug)
        run: |
          python -V
          python -c "import pandas as pd, yfinance as yf, numpy as np; print('pandas', pd.__version__, 'yfinance', yf.__version__, 'numpy', np.__version__)"

      - name: Build universe.csv from Trading 212 (fallback to stale)
        env:
          T212_API_KEY: ${{ secrets.T212_API_KEY }} # Auth token used to pull the broker’s instrument list.
          ALLOW_STALE: '1' # Lets the step succeed even if fresh data cannot be fetched.
        run: |
          python scripts/universe_from_trading_212.py || true
          ls -lah data || true
          ls -lah data/universe.csv || true

      - name: Clean universe (fallback to stale)
        env:
          T212_API_KEY: ${{ secrets.T212_API_KEY }} # Auth token reused for any cleanup calls that need it.
          CLEAN_ALLOW_OFFLINE: '1' # Skips network validation if offline so the pipeline keeps moving.
          ALLOW_STALE: '1' # Allows using previously saved universe files if refresh fails.
        run: |
          python scripts/clean_universe.py || true
          wc -l data/universe_clean.csv || true
          head -n 5 data/universe_clean.csv || true

      - name: Auto-build Yahoo aliases from last run’s rejects
        env:
          ALIASES_FROM: data/universe_rejects.csv # Reads tickers that failed to fetch last time as candidates for aliases.
          ALIASES_OUT: data/aliases.csv # Writes the generated alias mappings to this file for future runs.
          ALIASES_MAX_PER_RUN: '250' # Caps how many alias attempts are generated per run to stay efficient.
          ALIAS_LOG_LEVEL: 'INFO' # Controls verbosity of the alias builder so logs are readable.
          YF_TEST_PERIOD: '60d' # Uses 60 days of history to test whether an alias actually resolves.
          YF_TEST_INTERVAL: '1d' # Downloads daily bars for the alias smoke test to reduce rate limits.
        run: |
          python scripts/aliases_autobuild.py || true

      - name: Run notifier
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }} # Auth key used to call GPT for the final adjudication text.
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }} # Webhook used to post the daily alert to Discord.

          # --- Logging ---
          # --- Logging ---
          PYTHONUNBUFFERED: '1'
          FEATURES_LOG_LEVEL: 'INFO' # DEBUG only when diagnosing
          FEATURES_VERBOSE: '0'
          FEATURES_LOG_EVERY: '500'
          RANKER_LOG_LEVEL: 'DEBUG' # keep RobustRanker chatty
          RANKER_VERBOSE: '1'
          RANKER_LOG_EVERY: '300'
          QUICK_LOG_LEVEL: 'INFO'

          # --- yfinance ---
          YF_CHUNK_SIZE: '50' # slightly smaller batches reduce 404/retries
          YF_MAX_RETRIES: '5'
          YF_RETRY_SLEEP: '2.5'

          # --- Tiering ---
          TIER_POLICY: 'TOPK_ADV'
          TIER_TOPK_LARGE: '600' # a bit broader = less tier edge cases
          TIER_BACKFILL_UNKNOWN_AS: 'small'

          # --- Stage-1 quick pass ---
          STAGE1_MODE: 'normal' # a touch stricter than 'loose'
          STAGE1_KEEP: '200'
          STAGE1_MIN_SMALL: '100'
          STAGE1_MIN_LARGE: '100'
          STAGE1_RESCUE_FRAC: '0.10' # fewer borderline rescues
          STAGE1_WRITE_CSV: '0'

          # --- Stage-2 Top-10 (stratified) ---
          STAGE2_MIN_SMALL: '5'
          STAGE2_MIN_LARGE: '5'
          STAGE2_MIN_PE: '5'
          RANKER_PROFILE: 'C'
          # =============================================================================
          # Ranking Profiles  (env: RANKER_PROFILE or SELECTION_MODE)  →  'A' | 'B' | 'C'
          #
          # A = Aggressive  (momentum/trend-forward; looser stability caps)
          #   Weights:  w_trend=0.30, w_momo=0.30, w_struct=0.12, w_stab=0.08, w_blowoff=0.01, w_value=0.19
          #   Soft caps: atr_soft_cap=6.5, vol20_soft_cap=300, dd_soft_cap=-40
          #   Intent: chase strong trends and breakouts; tolerate more heat/volatility.
          #
          # B = Balanced (DEFAULT)  (your current defaults)
          #   Weights:  w_trend=0.28, w_momo=0.26, w_struct=0.14, w_stab=0.12, w_blowoff=0.02, w_value=0.18
          #   Soft caps: atr_soft_cap=6.0, vol20_soft_cap=250, dd_soft_cap=-35
          #   Intent: even mix of trend/momentum, structure, stability, and value tilt.
          #
          # C = Conservative  (value/stability-forward; harsher on overheated names)
          #   Weights:  w_trend=0.24, w_momo=0.18, w_struct=0.16, w_stab=0.20, w_blowoff=0.04, w_value=0.18
          #   Soft caps: atr_soft_cap=5.5, vol20_soft_cap=220, dd_soft_cap=-30
          #   Extras: a small guardrail trims ~8 pts when value is poor AND the chart is extended.
          #           (AVWAP premium headwind + FVA KO haircut apply in all profiles.)
          #
          # How to use:
          #   export RANKER_PROFILE=A   # Aggressive
          #   export RANKER_PROFILE=B   # Balanced (default)
          #   export RANKER_PROFILE=C   # Conservative
          # =============================================================================

          # --- Hard filters ---
          HARD_DROP_MODE: 'normal' # tougher on junk; still not 'strict'
          HARD_GRACE_ATR: '1.5'

          # --- Quick scorer knobs ---
          QS_PE_WEIGHT: '0.05' # de-emphasize PE a hair
          QS_PE_WEIGHT_LARGE: '0.08'
          QS_PE_WEIGHT_SMALL: '0.03'
          QS_USE_XS: '1'
          QS_USE_FVA: '1' # uses the anchor nudge we added

          # Optional (if you wired them in quick_scorer)
          QS_FVA_PEN_MAX: '20' # was 12; stronger penalty when price >> FVA
          QS_FVA_BONUS_MAX: '6' # bonus for price << FVA stays small/asymmetric
          QS_FVA_KO_PCT: '35' # heavy penalty if >35% above anchor AND extended
          QS_VAL_OVERLAY_MAX: '12' # cap for ratio overlay added into "struct"
          QS_STRUCT_PREM_CAP: '12' # stronger AVWAP premium headwind for stretched names

          # --- Small-cap liquidity dampening ---
          QS_SMALL_PRICE_MAX: '8'
          QS_SMALL_LIQ_MAX: '8000000'
          QS_MIN_DOLLAR_VOL_20D: '3000000'

          # --- P/E refine pass ---
          STAGE1_PE_RESCORE: '1'
          STAGE1_PE_POOL: '2000'

          # --- ATH guard ---
          ATH_GUARD: '1'
          ATH_NEAR_PCT: '1.0'
          ATH_MIN_RSI: '80'
          ATH_MIN_VS50: '25'
          ATH_VOL_RELIEF: '60'
          ATH_SCORE_HAIRCUT: '22'

          # --- OpenAI call timeout (seconds) ---
          OPENAI_TIMEOUT: '360' # Limits the GPT request to six minutes to avoid stalling the job.

          # --- Dump exact GPT inputs ---
          LOG_GPT_INPUT: '1' # Writes the exact per-ticker blocks that were sent to GPT for auditability.
          LOG_GPT_INPUT_STDOUT: '0' # Suppresses noisy console previews while still saving the file.
        run: |
          python scripts/notify.py

      - name: Find latest blocks file
        id: pick_file
        shell: bash
        run: |
          set -e
          FILE=$(ls -1t data/logs/blocks_to_gpt_*.txt 2>/dev/null | head -n1 || true)
          if [ -z "$FILE" ]; then
            echo "found=0" >> "$GITHUB_OUTPUT"
            echo "No blocks_to_gpt_*.txt found"
          else
            echo "found=1" >> "$GITHUB_OUTPUT"
            echo "file=$FILE" >> "$GITHUB_OUTPUT"
            echo "basename=$(basename "$FILE")" >> "$GITHUB_OUTPUT"
          fi

      - name: Commit blocks file to repo (overwrite single file)
        if: steps.pick_file.outputs.found == '1'
        shell: bash
        run: |
          set -e
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          mkdir -p logs
          cp "${{ steps.pick_file.outputs.file }}" "logs/blocks_to_gpt_latest.txt"
          git add "logs/blocks_to_gpt_latest.txt"
          if git diff --cached --quiet; then
            echo "No changes to commit."
          else
            git commit -m "Update latest GPT blocks (run #${{ github.run_number }})"
            git push
          fi
